{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS301 TubeSpam Classification Task 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEwtlsAkfwEJ",
        "outputId": "c3140c57-490b-4aef-e75a-ccb52e8c231c"
      },
      "source": [
        "# If you ran task 2 you don't need to re-clone the directory. You only need to mount your drive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # Mount your drive\n",
        "\n",
        "# Skip these lines below if you already cloned.\n",
        "%cd /content/drive/MyDrive/\n",
        "!git clone https://github.com/yusefmustafa/CS-301-Assignment-3.git # Clone my repo which has the TubeSpam dataset"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n",
            "Cloning into 'CS-301-Assignment-3'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 58 (delta 15), reused 52 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (58/58), done.\n",
            "Checking out files: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5TCohsOLuUd",
        "outputId": "b60e7f70-9f49-4cae-c882-c84451b85bde"
      },
      "source": [
        "!pip install lime # Install LIME"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.7/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.5.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt2FOIUhZ-jm",
        "outputId": "009245b7-c7fa-4db1-cab8-d3bb3f3a1cc3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "# Feel free to uncomment the other datasets and change directories as needed.\n",
        "# We included only one dataset as per TA's instructions.\n",
        "tube_spam_dataset_locations = ['/content/drive/MyDrive/CS-301-Assignment-3/TubeSpamDataset/Youtube01-Psy.csv']\n",
        "                              #  '/content/drive/MyDrive/CS-301-Assignment-3/TubeSpamDataset/Youtube02-KatyPerry.csv',\n",
        "                              #  '/content/drive/MyDrive/CS-301-Assignment-3/TubeSpamDataset/Youtube03-LMFAO.csv', \n",
        "                              #  '/content/drive/MyDrive/CS-301-Assignment-3/TubeSpamDataset/Youtube04-Eminem.csv', \n",
        "                              #  '/content/drive/MyDrive/CS-301-Assignment-3/TubeSpamDataset/Youtube05-Shakira.csv']\n",
        "\n",
        "print(\"Combining all datasets...\")\n",
        "data = []\n",
        "for dataset_location in tube_spam_dataset_locations:\n",
        "  data.append(pd.read_csv(dataset_location))\n",
        "  print(\"Added:  \" + dataset_location)\n",
        "\n",
        "data = pd.concat(data, axis=0, ignore_index=True) # Collect all data from CSVs into one dataframe\n",
        "\n",
        "X = list(data['CONTENT'])\n",
        "y = list(data['CLASS'])# 0 = ham, 1 = spam\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3) # Reserve 30% of the data for training\n",
        "\n",
        "# Create our bag of words using CountVectorizer\n",
        "print(\"\\nCreating bag-of-words...\")\n",
        "vectorizer = CountVectorizer(analyzer = \"word\",\n",
        "                             tokenizer = None,\n",
        "                             preprocessor = None,\n",
        "                             stop_words = None,\n",
        "                             ngram_range = (1, 3),\n",
        "                             max_features = 1000) \n",
        "\n",
        "train_data_features = vectorizer.fit_transform(X_train)\n",
        "train_data_features = train_data_features.toarray()\n",
        "\n",
        "print(\"Found words: \" + str(vectorizer.get_feature_names()))\n",
        "print(\"Counting word frequency... \")\n",
        "# We can determine how important a word is to a document with a frequency counter\n",
        "tfidf = TfidfTransformer()\n",
        "tfidf_features = tfidf.fit_transform(train_data_features).toarray()\n",
        "\n",
        "# Train logistic regression model\n",
        "print(\"Training logistic regression model... \")\n",
        "lr = LogisticRegression()\n",
        "lr.fit(tfidf_features, y_train)\n",
        "\n",
        "print(\"Testing model against X_test...\")\n",
        "test_data_features = vectorizer.transform(X_test).toarray()\n",
        "test_data_tfidf_features = tfidf.fit_transform(test_data_features).toarray()\n",
        "\n",
        "predicted_y = lr.predict(test_data_tfidf_features)\n",
        "is_y_prediction_correct = predicted_y == y_test\n",
        "\n",
        "spam_detection_accuracy = np.mean(is_y_prediction_correct) * 100\n",
        "print ('Accuracy: ' + str(spam_detection_accuracy) + '%')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Combining all datasets...\n",
            "Added:  /content/drive/MyDrive/CS-301-Assignment-3/TubeSpamDataset/Youtube01-Psy.csv\n",
            "\n",
            "Creating bag-of-words...\n",
            "Found words: ['000', '000 000', '000 000 000', '100', '100 subscribers', '100 subscribers will', '127', '1415297812', '2014', '2015', '2billion', '2billion hits', '48051', '5million', '8bit', 'about', 'about to', 'absolutely', 'after', 'again', 'alive', 'all', 'all about', 'all of', 'alone', 'also', 'am', 'am new', 'am new youtuber', 'amazon', 'amp', 'amp theater', 'an', 'and', 'and check', 'and check out', 'and even', 'and get', 'and give', 'and please', 'and please subscribe', 'and the', 'and will', 'and will subscribe', 'any', 'any other', 'anymore', 'anyone', 'appreciate', 'appreciate if', 'apps', 'are', 'as', 'asia', 'asian', 'ass', 'at', 'at salon', 'at the', 'auditioning', 'auditioning for', 'away', 'awesome', 'awesome and', 'back', 'be', 'beautiful', 'because', 'because they', 'before', 'best', 'billion', 'billion views', 'billion views this', 'bing', 'but', 'buy', 'by', 'came', 'came here', 'came here to', 'came to', 'can', 'can see', 'can you', 'can you check', 'card', 'cards', 'care', 'care about', 'chance', 'chance to', 'change', 'change org', 'channel', 'channel and', 'channel and will', 'channel please', 'check', 'check it', 'check it out', 'check me', 'check me out', 'check my', 'check my channel', 'check out', 'check out my', 'check out the', 'check the', 'check the views', 'checked', 'checking', 'checking the', 'checking the views', 'chinese', 'ching', 'click', 'clothes', 'co', 'co k6a5xt', 'code', 'com', 'com pages', 'com ref', 'come', 'come and', 'come check', 'comment', 'comment is', 'comment tell', 'comments', 'company', 'content', 'cool', 'could', 'count', 'covers', 'dance', 'dance psy', 'day', 'dick', 'did', 'dislike', 'dislikes', 'do', 'do not', 'does', 'does this', 'does this have', 'does this video', 'don', 'don have', 'don miss', 'don miss out', 'done', 'dont', 'dont even', 'dont even understand', 'download', 'earn', 'earn money', 'earn money by', 'earth', 'editor', 'enjoy', 'epic', 'eugenekalinin', 'even', 'even understand', 'ever', 'every', 'everyone', 'fablife', 'face', 'facebook', 'facebook com', 'facebook com pages', 'family', 'feedback', 'feedback on', 'feedback on our', 'first', 'first billion', 'first comment', 'follow', 'follow me', 'follow me on', 'football', 'for', 'for free', 'for more', 'for the', 'for this', 'for you', 'free', 'free gift', 'free gift cards', 'friend', 'from', 'fuck', 'fucking', 'fuego', 'fuego la', 'fuego la la', 'funny', 'funny thing', 'funny thing is', 'game', 'gangnam', 'gangnam style', 'gangnam style 강남스타일', 'get', 'get free', 'get free gift', 'get it', 'get it you', 'get my', 'get to', 'gets', 'gets old', 'getting', 'gift', 'gift card', 'gift cards', 'girls', 'give', 'go', 'go here', 'go to', 'goal', 'gofundme', 'gofundme com', 'going', 'going to', 'going to be', 'good', 'google', 'google com', 'got', 'great', 'great day', 'group', 'gt', 'gt gt', 'gt gt gt', 'guy', 'guys', 'guys can', 'guys can you', 'guys please', 'gwar', 'gwar to', 'hackfbaccountlive', 'hackfbaccountlive com', 'hackfbaccountlive com ref', 'had', 'half', 'has', 'has more', 'hate', 'hate comments', 'have', 'have billion', 'having', 'he', 'he is', 'he saying', 'hello', 'help', 'her', 'here', 'here http', 'here http www', 'here to', 'here to check', 'hey', 'hey everyone', 'hey guys', 'hi', 'hi guys', 'history', 'hit', 'hit billion', 'hits', 'hits to', 'holy', 'holy shit', 'hoppa', 'how', 'how can', 'how does', 'how does this', 'how stupid', 'how this', 'http', 'http hackfbaccountlive', 'http hackfbaccountlive com', 'http image2you', 'http image2you ru', 'http www', 'http www gofundme', 'https', 'https soundcloud', 'https soundcloud com', 'https www', 'https www facebook', 'https www tsu', 'hyperurl', 'hyperurl co', 'hyperurl co k6a5xt', 'if', 'if reach', 'if reach 100', 'if there', 'if you', 'if you do', 'if you like', 'if you would', 'if your', 'im', 'im the', 'im the only', 'image2you', 'image2you ru', 'image2you ru 48051', 'improve', 'in', 'in 2014', 'in korea', 'in my', 'in my channel', 'in the', 'in the world', 'is', 'is about', 'is getting', 'is it', 'is so', 'is such', 'is the', 'isn', 'it', 'it has', 'it just', 'it out', 'it so', 'it would', 'it would be', 'it you', 'its', 'itunes', 'jackson', 'join', 'juno', 'juno wallet', 'just', 'just came', 'just came here', 'just checking', 'just dance', 'k6a5xt', 'know', 'know what', 'know you', 'korea', 'korean', 'la', 'la la', 'la la remix', 'la remix', 'la remix hyperurl', 'last', 'laugh', 'leave', 'leave some', 'like', 'like comment', 'like this', 'like to', 'likes', 'link', 'listen', 'listen to', 'little', 'little psy', 'live', 'll', 'll subscribe', 'lol', 'lol this', 'look', 'look at', 'lot', 'love', 'love it', 'loved', 'lt', 'mabey', 'made', 'made in', 'make', 'many', 'many views', 'me', 'me and', 'me for', 'me on', 'me on twitter', 'me out', 'me what', 'me what you', 'mean', 'memories', 'michael', 'michael jackson', 'million', 'millions', 'mind', 'minecraft', 'miss', 'miss out', 'mom', 'mon', 'mon billion', 'mon billion views', 'money', 'money by', 'money to', 'montages', 'more', 'more views', 'more views than', 'most', 'most viewed', 'most viewed video', 'most viewed youtube', 'much', 'music', 'music video', 'music videos', 'my', 'my channel', 'my channel and', 'my channel please', 'my content', 'my first', 'my music', 'my music videos', 'my name', 'my name is', 'my new', 'my self', 'my video', 'myself', 'name', 'name is', 'need', 'network', 'never', 'never gets', 'never gets old', 'new', 'new channel', 'new channel and', 'new social', 'new social network', 'new youtuber', 'new youtuber and', 'newest', 'news', 'next', 'nice', 'no', 'not', 'now', 'of', 'of shit', 'of the', 'of the views', 'of your', 'of youtube', 'old', 'omg', 'on', 'on earth', 'on how', 'on my', 'on my channel', 'on our', 'on the', 'on twitter', 'on youtube', 'one', 'only', 'only million', 'oppa', 'or', 'or not', 'org', 'other', 'our', 'our channel', 'out', 'out and', 'out my', 'out my music', 'out my new', 'out my rap', 'out my vidios', 'out of', 'out of 000', 'out of the', 'out of your', 'out our', 'out our channel', 'out pivot', 'out pivot animations', 'out please', 'out please do', 'out please subscribe', 'out put', 'out the', 'out there', 'over', 'over billion', 'over billion views', 'page', 'pages', 'part', 'people', 'people are', 'people can', 'people on', 'people on earth', 'people this video', 'people who', 'people who only', 'people who say', 'peoples', 'peoples could', 'peoples could check', 'perfect', 'perfect for', 'perfect for any', 'perform', 'perform the', 'perform the 2015', 'permpage', 'permpage lt', 'person', 'person alive', 'person alive after', 'person reading', 'person reading this', 'petition', 'petition youtube_corporation_fox_broadcasting_company_anular_os_strikes_no_canal_nostalgia', 'petition youtube_corporation_fox_broadcasting_company_anular_os_strikes_no_canal_nostalgia cxpzpgb', 'petitions', 'petitions the', 'petitions the national', 'phenomena', 'phenomena behind', 'phenomena behind it', 'photo', 'photo editor', 'photo editor download', 'photos', 'photos 1496273723978022', 'photos 1496273723978022 1073741828', 'photos ms', 'photos ms ejw9kvkoxdamqm808h5z', 'php', 'php 10200253113705769', 'php 10200253113705769 amp', 'pictures', 'pictures funnytortspics', 'pictures funnytortspics https', 'pictures if', 'pictures if not', 'piece', 'piece of', 'piece of shit', 'pivot', 'pivot animations', 'pivot animations in', 'place', 'place to', 'place to promote', 'plan', 'plan on', 'plan on auditioning', 'planet', 'planet we', 'planet we get', 'platform', 'platform where', 'platform where share', 'play', 'play 2015', 'play 2015 superbowl', 'play google', 'play google com', 'play itunes', 'play itunes or', 'play minecraft', 'play minecraft thanks', 'please', 'please also', 'please also subscribe', 'please and', 'please and listen', 'please check', 'please check out', 'please come', 'please come check', 'please come leave', 'please do', 'please do buy', 'please do singing', 'please feel', 'please feel free', 'please give', 'please give us', 'please guys', 'please guys can', 'please https', 'please https www', 'please leave', 'please leave like', 'please like', 'please like frigea', 'please like http', 'please like https', 'please my', 'please my android', 'please pray', 'please pray to', 'please subscribe', 'please subscribe am', 'please subscribe and', 'please subscribe fuego', 'please subscribe if', 'please subscribe me', 'please subscribe too', 'please throw', 'please throw sub', 'please xd', 'please xd lol', 'plizz', 'plizz withing', 'plus', 'plz', 'plz plz', 'plz plz plz', 'plz subscribe', 'plz subscribe to', 'popular', 'post', 'posting', 'posts', 'pray', 'pretty', 'pretty clothes', 'projects', 'psy', 'psy gangnam', 'psy gangnam style', 'psy is', 'qid', 'qid 1415297812', 'rap', 'raw', 're', 'reach', 'reach 100', 'reach 100 subscribers', 'reading', 'ready', 'real', 'really', 'really appreciate', 'really appreciate if', 'ref', 'remix', 'remix hyperurl', 'remix hyperurl co', 'rewards', 'right', 'right now', 'ru', 'ru 48051', 'sad', 'salon', 'salt', 'say', 'saying', 'school', 'section', 'see', 'see how', 'self', 'sexy', 'share', 'she', 'shirts', 'shit', 'should', 'should check', 'should check my', 'show', 'site', 'smart', 'so', 'so don', 'so fucking', 'so funny', 'so many', 'so many views', 'so much', 'so they', 'social', 'social network', 'some', 'some of', 'song', 'song is', 'song was', 'soon', 'soundcloud', 'soundcloud com', 'spam', 'sr', 'started', 'still', 'straight', 'stupid', 'sty', 'style', 'style 강남스타일', 'sub', 'sub to', 'sub to my', 'subs', 'subscribe', 'subscribe and', 'subscribe back', 'subscribe me', 'subscribe to', 'subscribe to me', 'subscribe to my', 'subscribers', 'subscribers will', 'such', 'sucks', 'super', 'suscribe', 'takes', 'teespring', 'teespring com', 'tell', 'tell me', 'tell me what', 'than', 'thanks', 'thanks https', 'that', 'that this', 'that this song', 'the', 'the best', 'the first', 'the first billion', 'the first comment', 'the funny', 'the funny thing', 'the little', 'the most', 'the most viewed', 'the only', 'the salt', 'the view', 'the view count', 'the views', 'the views are', 'the world', 'theater', 'them', 'then', 'there', 'there are', 'there be', 'there is', 'there there', 'these', 'they', 'they are', 'thing', 'thing is', 'think', 'think you', 'this', 'this has', 'this have', 'this is', 'this is the', 'this music', 'this shit', 'this song', 'this song is', 'this video', 'this video have', 'this video is', 'time', 'to', 'to be', 'to billion', 'to check', 'to check the', 'to help', 'to know', 'to me', 'to me for', 'to my', 'to my channel', 'to see', 'to the', 'to this', 'too', 'took', 'top', 'tops', 'try', 'tsu', 'tsu co', 'twitter', 'type', 'type amp', 'type amp theater', 'uk', 'understand', 'up', 'us', 'use', 'very', 'video', 'video have', 'video in', 'video in the', 'video is', 'video is so', 'video of', 'video of history', 'video on', 'video on youtube', 'video out', 'video to', 'videos', 'vids', 'view', 'view count', 'viewed', 'viewed video', 'viewed video in', 'viewed youtube', 'viewed youtube video', 'views', 'views and', 'views are', 'views holy', 'views holy shit', 'views if', 'views if there', 'views than', 'views this', 'views this song', 'wallet', 'want', 'want to', 'wanted', 'wanted to', 'was', 'was so', 'watch', 'watch it', 'watching', 'watching this', 'we', 'we get', 'we get it', 'we have', 'welcome', 'well', 'were', 'what', 'what he', 'what the', 'what you', 'what you think', 'when', 'where', 'while', 'who', 'why', 'why does', 'why does this', 'will', 'will be', 'will get', 'will subscribe', 'will subscribe back', 'win', 'with', 'world', 'world of', 'world of warcraft', 'world record', 'world record youtube', 'worldwide', 'worldwide don', 'worldwide don miss', 'worldwide_life', 'worth', 'worth funny', 'worth funny but', 'worthless', 'worthless fish', 'worthless fish head', 'would', 'would be', 'would be grateful', 'would be great', 'would be wonderful', 'would enjoy', 'would enjoy that', 'would like', 'would like my', 'would like to', 'would really', 'would really appreciate', 'would saw', 'would saw this', 'wouldnt', 'wouldnt mind', 'wouldnt mind chacking', 'wow', 'wow 23', 'wow 23 min', 'wow comments', 'wow comments section', 'wp', 'wp 1495323920744243', 'wp 1495323920744243 ref', 'wrap', 'wrap up', 'wrap up the', 'wrenn', 'wrenn almond', 'wrenn almond eyes', 'wrong', 'www', 'www change', 'www change org', 'www facebook', 'www facebook com', 'www gofundme', 'www gofundme com', 'www tsu', 'www tsu co', 'xd', 'years', 'years eve', 'years eve event', 'years later', 'years to', 'years to get', 'yeat', 'yeat only', 'yeat only get', 'you', 'you are', 'you came', 'you came here', 'you can', 'you check', 'you check my', 'you do', 'you guys', 'you hate', 'you like', 'you re', 'you think', 'you would', 'your', 'youtube', 'youtube and', 'youtube video', 'youtuber', 'youtuber and', 'zombie', '강남스타일']\n",
            "Counting word frequency... \n",
            "Training logistic regression model... \n",
            "Testing model against X_test...\n",
            "Accuracy: 98.09523809523809%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}